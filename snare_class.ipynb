{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading libraries\n",
    "import subprocess\n",
    "import joblib\n",
    "from Bio import SeqIO\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sequence(sequence_file, hmm_file):\n",
    "    evalues = {}\n",
    "    sequences = {}\n",
    "    \n",
    "    result = subprocess.run(['hmmsearch', \"--incE\", \"1000000\", hmm_file, sequence_file], text=True, capture_output=True, check=True)\n",
    "    output = result.stdout\n",
    "    if \"[No targets detected that satisfy reporting thresholds]\" in output:\n",
    "        return [], [], []\n",
    "    lines = output.split(\"E-value\")[3].split(\"Domain annotation for each sequence (and alignments):\")[0].split(\"\\n\")\n",
    "    \n",
    "    lines = lines[2:-3]\n",
    "    evalues = []\n",
    "    scores = []\n",
    "    sequences = []\n",
    "    for line in lines:\n",
    "        evalues.append(float(line.split()[0]))\n",
    "        sequences.append(line.split()[-1])\n",
    "        scores.append(float(line.split()[1]))\n",
    "    return evalues, sequences, scores\n",
    "\n",
    "def build_scoring_matrix(sequences_files, HMM_files):\n",
    "    scoring_matrix = {}\n",
    "    scoring_dfs = []\n",
    "    all_labels = []\n",
    "\n",
    "    for hmm_index, hmm_file in enumerate(HMM_files):\n",
    "        scoring_matrix[hmm_file.split(\".\")[0]] = {}\n",
    "        all_evalues = []\n",
    "        all_scores = []\n",
    "        row_names = []\n",
    "        labels = []\n",
    "        lengths = []\n",
    "        for seq_index, seq_file in enumerate(sequences_files):\n",
    "         \n",
    "            evalues, seq_names, scores = score_sequence(seq_file, hmm_file)\n",
    "            all_evalues.extend(evalues)\n",
    "            row_names.extend(seq_names)\n",
    "            all_scores.extend(scores)   \n",
    "            ground_truth = [seq_file.split(\".\")[0]] * len(evalues)\n",
    "            labels.extend(ground_truth)\n",
    "            for single_seq in SeqIO.parse(seq_file, \"fasta\"):\n",
    "                if single_seq.id in row_names:\n",
    "                    lengths.append(len(single_seq.seq))     \n",
    "\n",
    "        scoring_matrix[hmm_file.split(\".\")[0]][\"Evalue\"] = all_evalues\n",
    "        scoring_matrix[hmm_file.split(\".\")[0]][ \" score/length\"] = np.array(all_scores)/np.array(lengths)\n",
    "        scoring_matrix[hmm_file.split(\".\")[0]][\"labels\"] = labels\n",
    "        scoring_matrix[hmm_file.split(\".\")[0]][\"sequences\"] = row_names\n",
    "        \n",
    "    for key, value in scoring_matrix.items():\n",
    "        temp_df = pd.DataFrame(value).set_index('sequences')  # set 'names' as index\n",
    "        temp_df.columns = [f'{col}{key}' for col in temp_df.columns]  # rename columns\n",
    "        scoring_dfs.append(temp_df)\n",
    "    scoring_data_frame = pd.concat(scoring_dfs, axis=1)\n",
    "    #print(list(scoring_data_frame.keys()))\n",
    "# replace None values with np.nan\n",
    "    scoring_data_frame = scoring_data_frame.replace({None: np.nan})\n",
    " \n",
    "    column_label_names = [\"labels\"+hmm_file.split(\".\")[0] for file in sequences_files]\n",
    "    scoring_data_frame[\"labels\"] = None\n",
    "    scoring_data_frame[\"labels\"] = scoring_data_frame[column_label_names].bfill(axis=1).iloc[:, 0]\n",
    "    scoring_data_frame = scoring_data_frame.drop(column_label_names, axis=1)\n",
    "    return scoring_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNAREClassifier(seq_path, score_null_replace = 0.01, eval_null_replace = 0.9):\n",
    "    # prot_sequence = SeqIO.read(seq_path, \"fasta\")\n",
    "\n",
    "    # Loading the HMM models\n",
    "    HMM_files = ['Qa.hmm', 'Qb.hmm', 'Qc.hmm', 'R.hmm', 'Snap.hmm']\n",
    "\n",
    "    # Loading the random forest model\n",
    "    rf1 = joblib.load('rf_snap.pkl')\n",
    "\n",
    "    # Scoring the sequence\n",
    "    scoring_data_frame = build_scoring_matrix([seq_path], HMM_files)\n",
    "\n",
    "    # Data preprocessing\n",
    "\n",
    "    # Define the columns to fill null values in\n",
    "    eval_columns = [col for col in scoring_data_frame.columns if 'Evalue' in col]\n",
    "    score_columns = [col for col in scoring_data_frame.columns if ' score' in col]\n",
    "\n",
    "    # Fill null values in the evalue columns\n",
    "    for column in eval_columns:\n",
    "        scoring_data_frame[column] = scoring_data_frame[column].fillna(eval_null_replace)\n",
    "\n",
    "    # Fill null values in the score columns\n",
    "    for column in score_columns:\n",
    "        scoring_data_frame[column] = scoring_data_frame[column].fillna(score_null_replace)\n",
    "\n",
    "   # Define the class mapping\n",
    "    class_mapping1 = {0: 'Qa', 1: 'Qb', 2: 'Qc', 3: 'R', 4: 'SNAP'}\n",
    "\n",
    "    # Predicting the labels\n",
    "    columns_to_drop = ['labels', 'labelsQa', 'labelsQb', 'labelsQc', 'labelsR']\n",
    "    prediction1 = rf1.predict(scoring_data_frame.drop(columns_to_drop, axis=1))\n",
    "    prediction_proba1 = rf1.predict_proba(scoring_data_frame.drop(columns_to_drop, axis=1))\n",
    "\n",
    "    if prediction1[0] == 0:\n",
    "        HMM_files = ['blueQa.hmm', 'greenQa.hmm', 'redQa.hmm', 'pinkOr.hmm', 'purpleQa.hmm', 'yellowQa.hmm']\n",
    "        rfQa2 = joblib.load('rfQa2.pkl')\n",
    "\n",
    "        scoring_data_frame = build_scoring_matrix([seq_path], HMM_files)\n",
    "\n",
    "        rename_dictQa = {\n",
    "            ' score/lengthblueQa': ' score/lengthblue',\n",
    "            ' score/lengthgreenQa': ' score/lengthgreen',\n",
    "            ' score/lengthpinkOrQa': ' score/lengthpinkOr',\n",
    "            ' score/lengthpurpleQa': ' score/lengthpurple',\n",
    "            ' score/lengthredQa': ' score/lengthred',\n",
    "            ' score/lengthyellowQa': ' score/lengthyellow',\n",
    "            'EvalueblueQa': 'Evalueblue',\n",
    "            'EvaluegreenQa': 'Evaluegreen',\n",
    "            'EvaluepinkOrQa': 'EvaluepinkOr',\n",
    "            'EvaluepurpleQa': 'Evaluepurple',\n",
    "            'EvalueredQa': 'Evaluered',\n",
    "            'EvalueyellowQa': 'Evalueyellow'\n",
    "        }\n",
    "\n",
    "        scoring_data_frame = scoring_data_frame.rename(columns=rename_dictQa)\n",
    "\n",
    "        eval_columns = [col for col in scoring_data_frame.columns if 'Evalue' in col]\n",
    "        score_columns = [col for col in scoring_data_frame.columns if ' score' in col]\n",
    "\n",
    "        for column in eval_columns:\n",
    "            scoring_data_frame[column] = scoring_data_frame[column].fillna(eval_null_replace)\n",
    "        for column in score_columns:\n",
    "            scoring_data_frame[column] = scoring_data_frame[column].fillna(score_null_replace)\n",
    "\n",
    "        columns_to_dropQa = ['labels', 'labelsblueQa', 'labelsgreenQa', 'labelspinkOr', 'labelspurpleQa', 'labelsredQa']\n",
    "        predictionQa = rfQa2.predict(scoring_data_frame.drop(columns_to_dropQa, axis=1))\n",
    "        prediction_probaQa = rfQa2.predict_proba(scoring_data_frame.drop(columns_to_dropQa, axis=1))\n",
    "\n",
    "        class_mappingQa = {0: 'IIIb', 1: 'IIIa', 2: 'IV', 3: 'IVFungi', 4: 'I', 5: 'II'}\n",
    "        class_label2 = class_mappingQa[predictionQa[0]]\n",
    "        class_proba2 = prediction_probaQa[0, predictionQa[0]]\n",
    "\n",
    "    elif prediction1[0] == 1:\n",
    "        HMM_files = ['blueQb.hmm', 'greenQb.hmm', 'redQb.hmm', 'purpleQb.hmm']\n",
    "        rfQb = joblib.load('rfQb.pkl')\n",
    "\n",
    "        scoring_data_frame = build_scoring_matrix([seq_path], HMM_files)\n",
    "\n",
    "        rename_dictQb = {\n",
    "            ' score/lengthblueQb': ' score/lengthblue',\n",
    "            ' score/lengthgreenQb': ' score/lengthgreen',\n",
    "            ' score/lengthpurpleQb': ' score/lengthpurple',\n",
    "            ' score/lengthredQb': ' score/lengthred',\n",
    "            'EvalueblueQb': 'Evalueblue',\n",
    "            'EvaluegreenQb': 'Evaluegreen',\n",
    "            'EvaluepurpleQb': 'Evaluepurple',\n",
    "            'EvalueredQb': 'Evaluered'\n",
    "        }\n",
    "\n",
    "        scoring_data_frame = scoring_data_frame.rename(columns=rename_dictQb)\n",
    "\n",
    "        eval_columns = [col for col in scoring_data_frame.columns if 'Evalue' in col]\n",
    "        score_columns = [col for col in scoring_data_frame.columns if ' score' in col]\n",
    "\n",
    "        for column in eval_columns:\n",
    "            scoring_data_frame[column] = scoring_data_frame[column].fillna(eval_null_replace)\n",
    "\n",
    "        for column in score_columns:\n",
    "            scoring_data_frame[column] = scoring_data_frame[column].fillna(score_null_replace)\n",
    "\n",
    "        columns_to_dropQb = ['labels', 'labelsblueQb', 'labelsgreenQb', 'labelsredQb']\n",
    "        predictionQb = rfQb.predict(scoring_data_frame.drop(columns_to_dropQb, axis=1))\n",
    "        prediction_probaQb = rfQb.predict_proba(scoring_data_frame.drop(columns_to_dropQb, axis=1))\n",
    "\n",
    "        class_mappingQb = {0: 'II ER-Golgi', 1: 'IIIb', 2: 'I', 3: 'II Intra-Golgi'}\n",
    "        class_label2 = class_mappingQb[predictionQb[0]]\n",
    "        class_proba2 = prediction_probaQb[0, predictionQb[0]]\n",
    "\n",
    "    elif prediction1[0] == 2:\n",
    "        HMM_files = ['blueQc.hmm', 'purpleQc.hmm', 'redQc.hmm', 'greenQc.hmm']\n",
    "        rfQc = joblib.load('rfQc.pkl')\n",
    "\n",
    "        scoring_data_frame = build_scoring_matrix([seq_path], HMM_files)\n",
    "\n",
    "        rename_dictQc = {\n",
    "            ' score/lengthblueQc': ' score/lengthblue',\n",
    "            ' score/lengthgreenQc': ' score/lengthgreen',\n",
    "            ' score/lengthpurpleQc': ' score/lengthpurple',\n",
    "            ' score/lengthredQc': ' score/lengthred',\n",
    "            'EvalueblueQc': 'Evalueblue',\n",
    "            'EvaluegreenQc': 'Evaluegreen',\n",
    "            'EvaluepurpleQc': 'Evaluepurple',\n",
    "            'EvalueredQc': 'Evaluered'\n",
    "        }\n",
    "\n",
    "        scoring_data_frame = scoring_data_frame.rename(columns=rename_dictQc)\n",
    "\n",
    "        eval_columns = [col for col in scoring_data_frame.columns if 'Evalue' in col]\n",
    "        score_columns = [col for col in scoring_data_frame.columns if ' score' in col]\n",
    "\n",
    "        for column in eval_columns:\n",
    "            scoring_data_frame[column] = scoring_data_frame[column].fillna(eval_null_replace)\n",
    "\n",
    "        for column in score_columns:\n",
    "            scoring_data_frame[column] = scoring_data_frame[column].fillna(score_null_replace)\n",
    "\n",
    "        columns_to_dropQc = ['labels', 'labelsblueQc', 'labelspurpleQc', 'labelsredQc']\n",
    "        predictionQc = rfQc.predict(scoring_data_frame.drop(columns_to_dropQc, axis=1))\n",
    "        prediction_probaQc = rfQc.predict_proba(scoring_data_frame.drop(columns_to_dropQc, axis=1))\n",
    "\n",
    "        class_mappingQc = {0: 'I', 1: 'IIIc', 2: 'IIIb', 3: 'II'}\n",
    "        class_label2 = class_mappingQc[predictionQc[0]]\n",
    "        class_proba2 = prediction_probaQc[0, predictionQc[0]]\n",
    "\n",
    "\n",
    "    \n",
    "    elif prediction1[0] == 3:\n",
    "        HMM_files = ['yellowR.hmm', 'purpleR.hmm', 'redR.hmm', 'blueR.hmm', 'greenR.hmm']\n",
    "        rfR = joblib.load('rfR.pkl')\n",
    "\n",
    "        scoring_data_frame = build_scoring_matrix([seq_path], HMM_files)\n",
    "\n",
    "        rename_dictR = {\n",
    "            ' score/lengthblueR': ' score/lengthblue',\n",
    "            ' score/lengthgreenR': ' score/lengthgreen',\n",
    "            ' score/lengthpurpleR': ' score/lengthpurple',\n",
    "            ' score/lengthredR': ' score/lengthred',\n",
    "            ' score/lengthyellowR': ' score/lengthyellow',\n",
    "            'EvalueblueR': 'Evalueblue',\n",
    "            'EvaluegreenR': 'Evaluegreen',\n",
    "            'EvaluepurpleR': 'Evaluepurple',\n",
    "            'EvalueredR': 'Evaluered',\n",
    "            'EvalueyellowR': 'Evalueyellow'\n",
    "        }\n",
    "\n",
    "        # Rename the columns\n",
    "        scoring_data_frame = scoring_data_frame.rename(columns=rename_dictR)\n",
    "\n",
    "        eval_columns = [col for col in scoring_data_frame.columns if 'Evalue' in col]\n",
    "        score_columns = [col for col in scoring_data_frame.columns if ' score' in col]\n",
    "\n",
    "        for column in eval_columns:\n",
    "            scoring_data_frame[column] = scoring_data_frame[column].fillna(eval_null_replace)\n",
    "\n",
    "        for column in score_columns:\n",
    "            scoring_data_frame[column] = scoring_data_frame[column].fillna(score_null_replace)\n",
    "\n",
    "\n",
    "        columns_to_dropR = ['labels', 'labelsyellowR', 'labelspurpleR', 'labelsredR', 'labelsblueR']\n",
    "        predictionR = rfR.predict(scoring_data_frame.drop(columns_to_dropR, axis=1))\n",
    "        prediction_probaR = rfR.predict_proba(scoring_data_frame.drop(columns_to_dropR, axis=1))\n",
    "\n",
    "        class_mappingR = {0: 'III', 1: 'Reg', 2: 'II', 3: 'IV', 4: 'I'}\n",
    "        class_label2 = class_mappingR[predictionR[0]]\n",
    "        class_proba2 = prediction_probaR[0, predictionR[0]]\n",
    "\n",
    "    elif prediction1[0] == 4:\n",
    "        HMM_files = ['blueSnap.hmm', 'greenSnap.hmm', 'redSnap.hmm']\n",
    "        rfSnap = joblib.load('rfSnap.pkl')\n",
    "        scoring_data_frame = build_scoring_matrix([seq_path], HMM_files)\n",
    "        \n",
    "        rename_dictSnap = {\n",
    "            ' score/lengthblueSnap': ' score/lengthblue',\n",
    "            ' score/lengthgreenSnap': ' score/lengthgreen',\n",
    "            ' score/lengthredSnap': ' score/lengthred',\n",
    "            'EvalueblueSnap': 'Evalueblue',\n",
    "            'EvaluegreenSnap': 'Evaluegreen',\n",
    "            'EvalueredSnap': 'Evaluered'\n",
    "        }\n",
    "        scoring_data_frame = scoring_data_frame.rename(columns=rename_dictSnap)\n",
    "        eval_columns = [col for col in scoring_data_frame.columns if 'Evalue' in col]\n",
    "        score_columns = [col for col in scoring_data_frame.columns if ' score' in col]\n",
    "        for column in eval_columns:\n",
    "            scoring_data_frame[column] = scoring_data_frame[column].fillna(eval_null_replace)\n",
    "\n",
    "        for column in score_columns:\n",
    "            scoring_data_frame[column] = scoring_data_frame[column].fillna(score_null_replace)\n",
    "\n",
    "        columns_to_dropSnap = ['labels', 'labelsblueSnap', 'labelsgreenSnap']\n",
    "        predictionSnap = rfSnap.predict(scoring_data_frame.drop(columns_to_dropSnap, axis=1))\n",
    "        prediction_probaSnap = rfSnap.predict_proba(scoring_data_frame.drop(columns_to_dropSnap, axis=1))\n",
    "        class_mappingSnap = {0: 'SN29', 1: 'sec9 Fungi', 2: 'SN25'}\n",
    "        class_label2 = class_mappingSnap[predictionSnap[0]]\n",
    "        class_proba2 = prediction_probaSnap[0, predictionSnap[0]]\n",
    "\n",
    "        \n",
    "\n",
    "    # Get the class label\n",
    "    class_label1 = class_mapping1[prediction1[0]]\n",
    "\n",
    "    # Get the probability of the predicted class\n",
    "    class_proba1 = prediction_proba1[0, prediction1[0]]\n",
    "\n",
    "    return f\"Main group: {class_label1} with probability {class_proba1},  Sub group: {class_label2} with probability {class_proba2}\"\n",
    "    \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNAREClassifier(\"testQa.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def classify_sequences(fasta_file):\n",
    "    # Read the fasta file\n",
    "    sequences = SeqIO.parse(fasta_file, \"fasta\")\n",
    "\n",
    "    # Apply the SNAREClassifier function to each sequence\n",
    "    for sequence in sequences:\n",
    "        # Delete the temporary fasta file if it exists\n",
    "        if os.path.exists(\"temp.fasta\"):\n",
    "            os.remove(\"temp.fasta\")\n",
    "        \n",
    "        # Write the sequence to a temporary fasta file\n",
    "        with open(\"temp.fasta\", \"w\") as temp_file:\n",
    "            SeqIO.write(sequence, temp_file, \"fasta\")\n",
    "        \n",
    "        # Apply the SNAREClassifier function\n",
    "        result = SNAREClassifier(\"temp.fasta\")\n",
    "        \n",
    "        # Print or store the result\n",
    "        print(f\"Result for sequence {sequence.id}: {result}\\n \")\n",
    "\n",
    "classify_sequences(\"test.fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
